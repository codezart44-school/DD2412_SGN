{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # provides functions for activation, loss etc.\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.models.wide_resnet_28_2 import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = './checkpoints'\n",
    "DATA_DIR = './data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the Data Loader for FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_matrix(matrix):\n",
    "    s = [[str(round(e, 4)) for e in row] for row in matrix]\n",
    "    lens = [max(map(len, col)) for col in zip(*s)]\n",
    "    fmt = '\\t'.join('{{:{}}}'.format(x) for x in lens)\n",
    "    table = [fmt.format(*row) for row in s]\n",
    "    print('\\n'.join(table))\n",
    "\n",
    "\n",
    "#================================================= Fashion MNIST =============================================#\n",
    "\n",
    "class NoisyFashionMNIST(Dataset):\n",
    "    def __init__(self, dataset, noise_type=None, noise_rate=0.0, transition_matrix=None):\n",
    "        \"\"\"\n",
    "        A wrapper around the FashionMNIST dataset to add optional noise to the labels.\n",
    "\n",
    "        Args:\n",
    "            dataset (torch.utils.data.Dataset): The original FashionMNIST dataset.\n",
    "            noise_type (str, optional): Type of noise to apply. Options: 'symmetric', 'asymmetric'. Default is None.\n",
    "            noise_rate (float): The proportion of labels to corrupt (between 0 and 1).\n",
    "            transition_matrix (np.ndarray, optional): A custom transition probability matrix for asymmetric noise.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.noise_type = noise_type\n",
    "        self.noise_rate = noise_rate\n",
    "        self.num_classes = len(DataLoaderFashionMNIST.label_dict)\n",
    "        self.transition_matrix = transition_matrix\n",
    "        self._apply_noise()\n",
    "\n",
    "    def _apply_noise(self):\n",
    "        \"\"\"\n",
    "        Applies the specified noise to the dataset's labels.\n",
    "        \"\"\"\n",
    "        if self.noise_type is None or self.noise_rate <= 0.0:\n",
    "            # No noise to apply\n",
    "            self.noisy_labels = self.dataset.targets.clone()  # Keep original labels\n",
    "            return\n",
    "\n",
    "        targets = self.dataset.targets.numpy()  # Convert to NumPy for easy manipulation\n",
    "        n_samples = len(targets)\n",
    "\n",
    "        if self.noise_type == 'symmetric':\n",
    "            # Symmetric noise: Replace labels with random labels with uniform probability\n",
    "            n_noisy = int(self.noise_rate * n_samples)\n",
    "            noisy_indices = np.random.choice(n_samples, n_noisy, replace=False)\n",
    "            noisy_labels = targets.copy()\n",
    "            for idx in noisy_indices:\n",
    "                original_label = targets[idx]\n",
    "                possible_labels = [label for label in range(self.num_classes) if label != original_label]\n",
    "                noisy_label = np.random.choice(possible_labels)\n",
    "                noisy_labels[idx] = noisy_label\n",
    "            self.noisy_labels = torch.tensor(noisy_labels, dtype=torch.long)\n",
    "\n",
    "        elif self.noise_type == 'asymmetric':\n",
    "            if self.transition_matrix is None:\n",
    "                # Generate a default transition matrix if not provided\n",
    "                self.transition_matrix = self._generate_default_transition_matrix()\n",
    "\n",
    "            # Ensure the transition matrix is properly normalized\n",
    "            assert self.transition_matrix.shape == (self.num_classes, self.num_classes)\n",
    "            assert np.allclose(self.transition_matrix.sum(axis=1), 1), \"Rows must sum to 1\"\n",
    "\n",
    "            noisy_labels = targets.copy()\n",
    "            for i in range(len(targets)):\n",
    "                true_label = targets[i]\n",
    "                noisy_labels[i] = np.random.choice(\n",
    "                    self.num_classes, p=self.transition_matrix[true_label]\n",
    "                )\n",
    "            self.noisy_labels = torch.tensor(noisy_labels, dtype=torch.long)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown noise type: {self.noise_type}\")\n",
    "\n",
    "    def _generate_default_transition_matrix(self):\n",
    "        \"\"\"\n",
    "        Generates a default transition matrix for asymmetric noise.\n",
    "        \"\"\"\n",
    "        num_classes = self.num_classes\n",
    "        matrix = np.eye(num_classes) * (1 - self.noise_rate)  # Start with identity matrix for correct labels\n",
    "\n",
    "        # Define plausible misclassification probabilities manually\n",
    "        plausible_transitions = {\n",
    "            0: [2, 6], # T-shirt -> Pullover, shirt\n",
    "            1: [6], # Trouser -> Shirt\n",
    "            2: [0, 6], # Pullover -> T-shirt, shirt\n",
    "            3: [4],  # Dress -> Coat\n",
    "            4: [3],  # Coat -> Dress,\n",
    "            5: [9], # Sandal -> Ankle boot\n",
    "            6: [0, 2],  # Shirt -> T-shirt, pullover,\n",
    "            7: [5], # Sneaker -> Sandal\n",
    "            8: [7], # Bag -> Sneaker\n",
    "            9: [5] # Ankle Boot -> Sandal \n",
    "        }\n",
    "\n",
    "        # Assign higher probabilities for plausible transitions\n",
    "        for i in range(num_classes):\n",
    "            for j in range(num_classes):\n",
    "                if i != j:  # Off-diagonal\n",
    "                    \n",
    "                    if i in plausible_transitions:\n",
    "                        transition_prob_factor = 0.7\n",
    "                        if j in plausible_transitions.get(i):\n",
    "                            matrix[i, j] = self.noise_rate * transition_prob_factor / (len(plausible_transitions.get(i)))\n",
    "                        else:\n",
    "                            matrix[i, j] = self.noise_rate * (1-transition_prob_factor) / (num_classes - (1 + len(plausible_transitions.get(i))))\n",
    "\n",
    "                    else:\n",
    "                        matrix[i, j] = self.noise_rate / (num_classes - 1)\n",
    "\n",
    "        # Normalize rows to sum to 1\n",
    "        #matrix = matrix / matrix.sum(axis=1, keepdims=True)\n",
    "        print_matrix(matrix)\n",
    "        return matrix\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, _ = self.dataset[index]  # Get the original image\n",
    "        label = self.noisy_labels[index]  # Get the noisy label\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class DataLoaderFashionMNIST:\n",
    "    label_dict = [\n",
    "        \"T-shirt/top\", # 0\n",
    "        \"Trouser\", # 1\n",
    "        \"Pullover\", # 2\n",
    "        \"Dress\", # 3\n",
    "        \"Coat\", # 4\n",
    "        \"Sandal\", # 5\n",
    "        \"Shirt\", # 6\n",
    "        \"Sneaker\", # 7\n",
    "        \"Bag\", # 8\n",
    "        \"Ankle boot\", # 9\n",
    "    ]\n",
    "\n",
    "    # Transformations\n",
    "    transform_train = transforms.Compose([transforms.ToTensor(),]) # Only one layer (no RGB), already standardized values\n",
    "    transform_test  = transforms.Compose([transforms.ToTensor(),]) # NOTE! NO AUGMENTATION ALLOWED\n",
    "\n",
    "    @classmethod\n",
    "    def get_loaders(\n",
    "        cls, \n",
    "        root='./data/',\n",
    "        download=True,\n",
    "        transform_train=transform_train, \n",
    "        batch_size=128,\n",
    "        num_workers=2,\n",
    "        noise_type=None,\n",
    "        noise_rate=0.0\n",
    "        ) -> tuple[torch.utils.data.DataLoader]:\n",
    "        \"\"\"\n",
    "        A getter function that returns both the train and test `torch.utils.data.DataLoader` objects in a tuple. \n",
    "\n",
    "        ...\n",
    "\n",
    "        Args:\n",
    "            root (str): The path to the directory where the FashionMNIST data is downloaded.\n",
    "            transform_train (torchvision.transform.Compose): A list of transforms to apply in sequence to the dataset data.\n",
    "            batch_size (int): The number of samples included in each batch. \n",
    "            noise_type (str): Type of noise to apply to train labels. Options: 'symmetric', 'asymmetric'.\n",
    "            noise_rate (float): The proportion of labels to corrupt (between 0 and 1).\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.utils.data.DataLoader]: A tuple of `DataLoader` objects for the train and test datasets. (train, test)\n",
    "        \"\"\"\n",
    "        assert isinstance(transform_train, transforms.Compose)\n",
    "\n",
    "        # Original datasets\n",
    "        dataset_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=download, transform=transform_train)\n",
    "        dataset_test  = torchvision.datasets.FashionMNIST(root=root, train=False, download=download, transform=cls.transform_test)\n",
    "\n",
    "        input_shape = dataset_train[0][0].shape\n",
    "\n",
    "        if noise_type is not None and noise_rate > 0.0:\n",
    "            dataset_train = NoisyFashionMNIST(dataset=dataset_train, noise_type=noise_type, noise_rate=noise_rate)\n",
    "\n",
    "        # Data loaders\n",
    "        dataloader_train = torch.utils.data.DataLoader(dataset=dataset_train, shuffle=True, batch_size=batch_size, num_workers=num_workers)\n",
    "        dataloader_test  = torch.utils.data.DataLoader(dataset=dataset_test, shuffle=False, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "        return (dataloader_train, dataloader_test, input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "dataloader_train, dataloader_test, input_shape = DataLoaderFashionMNIST.get_loaders(\n",
    "    root=DATA_DIR,\n",
    "    download=True,\n",
    "    batch_size=128,\n",
    "    num_workers=2,\n",
    "    noise_type=None, \n",
    "    noise_rate=0.0\n",
    "    )\n",
    "\n",
    "for batch in dataloader_train:\n",
    "    images, labels = batch\n",
    "    # plot first image sample\n",
    "    plt.imshow(images[0].squeeze())\n",
    "    plt.title(f'Label: {DataLoaderFashionMNIST.label_dict[labels[0]]} = {labels[0]}')\n",
    "    plt.show()\n",
    "    break\n",
    "\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the ResNet model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, conv_l2=0.0, bn_l2=0.0, version=1):\n",
    "        super().__init__()\n",
    "        self.version = version\n",
    "        self.conv_l2 = conv_l2\n",
    "        self.bn_l2 = bn_l2\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels, eps=1e-5, momentum=0.9)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels, eps=1e-5, momentum=0.9)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        self.shortcut = nn.Identity()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        l2_penalty = 0.0\n",
    "\n",
    "        if self.version == 2:\n",
    "            out = F.relu(self.bn1(x))\n",
    "            shortcut = self.shortcut(out)\n",
    "        else:\n",
    "            shortcut = self.shortcut(x)\n",
    "            out = F.relu(self.bn1(x))\n",
    "\n",
    "        out = self.conv1(out)\n",
    "        l2_penalty += self._compute_l2_penalty(self.conv1, self.conv_l2)\n",
    "\n",
    "        out = self.bn2(out)\n",
    "        l2_penalty += self._compute_l2_penalty(self.bn2, self.bn_l2)\n",
    "\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "        l2_penalty += self._compute_l2_penalty(self.conv2, self.conv_l2)\n",
    "\n",
    "        out += shortcut\n",
    "        if self.version == 1:\n",
    "            out = F.relu(out)\n",
    "\n",
    "        return out, l2_penalty\n",
    "\n",
    "    def _compute_l2_penalty(self, layer, l2_factor):\n",
    "        if hasattr(layer, 'weight') and layer.weight.requires_grad and l2_factor > 0.0:\n",
    "            return l2_factor * layer.weight.pow(2).sum()\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, depth, width_multiplier, num_classes=10, input_shape=(3, 32, 32), version=1, l2=0.0):\n",
    "        super(WideResNet, self).__init__()\n",
    "        assert (depth - 4) % 6 == 0, \"Depth should be 6n+4.\"\n",
    "        num_blocks = (depth - 4) // 6\n",
    "\n",
    "        self.in_channels = 16\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], self.in_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        self.layer1 = self._make_group(16 * width_multiplier, num_blocks, stride=1, version=version, conv_l2=l2, bn_l2=l2)\n",
    "        self.layer2 = self._make_group(32 * width_multiplier, num_blocks, stride=2, version=version, conv_l2=l2, bn_l2=l2)\n",
    "        self.layer3 = self._make_group(64 * width_multiplier, num_blocks, stride=2, version=version, conv_l2=l2, bn_l2=l2)\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(64 * width_multiplier, eps=1e-5, momentum=0.9)\n",
    "        self.fc = nn.Linear(64 * width_multiplier, num_classes)\n",
    "\n",
    "    def _make_group(self, out_channels, num_blocks, stride, version, conv_l2, bn_l2):\n",
    "        layers = []\n",
    "        layers.append(ResnetBlock(self.in_channels, out_channels, stride, conv_l2, bn_l2, version))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResnetBlock(out_channels, out_channels, stride=1, conv_l2=conv_l2, bn_l2=bn_l2, version=version))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        total_l2_penalty = 0.0\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x, l2_penalty = self._apply_block(self.layer1, x)\n",
    "        total_l2_penalty += l2_penalty\n",
    "\n",
    "        x, l2_penalty = self._apply_block(self.layer2, x)\n",
    "        total_l2_penalty += l2_penalty\n",
    "\n",
    "        x, l2_penalty = self._apply_block(self.layer3, x)\n",
    "        total_l2_penalty += l2_penalty\n",
    "\n",
    "        x = F.relu(self.bn(x))\n",
    "        x = F.adaptive_avg_pool2d(x, 1)  # Global average pooling\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x, total_l2_penalty\n",
    "\n",
    "    def _apply_block(self, block, x):\n",
    "        total_l2_penalty = 0.0\n",
    "        for layer in block:\n",
    "            x, l2_penalty = layer(x)\n",
    "            total_l2_penalty += l2_penalty\n",
    "        return x, total_l2_penalty\n",
    "\n",
    "\n",
    "def create_model(depth=28, width_multiplier=2, num_classes=10, input_shape=(3, 32, 32), version=1, l2=0.0):\n",
    "    \"\"\"Create the WideResNet model.\"\"\"\n",
    "    return WideResNet(depth, width_multiplier, num_classes, input_shape, version, l2)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Example usage\n",
    "    model = create_model(depth=28, width_multiplier=2, num_classes=10, version=1, l2=0.0)\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WideResNet(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): ResnetBlock(\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (shortcut): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    )\n",
      "    (1): ResnetBlock(\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (shortcut): Identity()\n",
      "    )\n",
      "    (2): ResnetBlock(\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (shortcut): Identity()\n",
      "    )\n",
      "    (3): ResnetBlock(\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (shortcut): Identity()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): ResnetBlock(\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (shortcut): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "    )\n",
      "    (1): ResnetBlock(\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (shortcut): Identity()\n",
      "    )\n",
      "    (2): ResnetBlock(\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (shortcut): Identity()\n",
      "    )\n",
      "    (3): ResnetBlock(\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (shortcut): Identity()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): ResnetBlock(\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (shortcut): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "    )\n",
      "    (1): ResnetBlock(\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (shortcut): Identity()\n",
      "    )\n",
      "    (2): ResnetBlock(\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (shortcut): Identity()\n",
      "    )\n",
      "    (3): ResnetBlock(\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (shortcut): Identity()\n",
      "    )\n",
      "  )\n",
      "  (bn): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.1\n",
    "SEED = 42\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs, l2_penalty = model(images)  # Outputs and L2 penalty\n",
    "        loss = criterion(outputs, labels) + l2_penalty  # Add L2 penalty to loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    accuracy = 100. * correct / total\n",
    "    return running_loss / len(loader), accuracy\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs, l2_penalty = model(images)  # Outputs and L2 penalty\n",
    "            loss = criterion(outputs, labels) + l2_penalty  # Add L2 penalty to loss\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    accuracy = 100. * correct / total\n",
    "    return running_loss / len(loader), accuracy\n",
    "\n",
    "\n",
    "def run():\n",
    "    # Data loaders\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    train_loader, test_loader, input_shape = DataLoaderFashionMNIST.get_loaders(\n",
    "        root=DATA_DIR,\n",
    "        download=False,\n",
    "        transform_train=transform,\n",
    "        batch_size=128,\n",
    "        num_workers=2,\n",
    "        noise_type=None,\n",
    "        noise_rate=0.0\n",
    "    )\n",
    "\n",
    "    # Model, Loss, Optimizer\n",
    "    model = create_model(depth=28, width_multiplier=2, num_classes=10, input_shape=input_shape, l2=5e-4).to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    # Main Training Loop\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "        \n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "        test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint_path = os.path.join(OUTPUT_DIR, f'checkpoint_epoch_{epoch + 1}.pth')\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 75\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader, criterion)\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[26], line 20\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     18\u001b[0m outputs, l2_penalty \u001b[38;5;241m=\u001b[39m model(images)  \u001b[38;5;66;03m# Outputs and L2 penalty\u001b[39;00m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels) \u001b[38;5;241m+\u001b[39m l2_penalty  \u001b[38;5;66;03m# Add L2 penalty to loss\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Desktop/DLAdv_SGN_GRP7/.venv/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/DLAdv_SGN_GRP7/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/DLAdv_SGN_GRP7/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
